{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Minor_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOhcdYwZ/nOHdcaBKFf9M2y"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oR7sNX3Q-jx6"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "import numpy as np\n",
        "\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "learning_rate   = 0.0008\n",
        "batch_size      = 4096\n",
        "sample_size     = 4096*5 \n",
        "epochs          = 10000  \n",
        "steps_per_epoch = int(sample_size/batch_size)\n",
        "\n",
        "# BOB_LOSS_THRESH = 0.02  # Exit when Bob loss < 0.02 and Eve > 7.7 bits\n",
        "# EVE_LOSS_THRESH = 7.7\n",
        "\n",
        "\n",
        "# Input and output configuration.\n",
        "TEXT_SIZE = 16\n",
        "KEY_SIZE  = 16\n",
        "\n",
        "# Training parameters.\n",
        "ITERS_PER_ACTOR = 1\n",
        "EVE_MULTIPLIER = 2  # Train Eve 2x for every step of Alice/Bob\n",
        "\n",
        "# Set a random seed to help reproduce the output\n",
        "seed = 7919\n",
        "tf.set_random_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# False if we want to train from scratch and true to contiune training a already trained model\n",
        "restore_trained_model = False\n",
        "\n",
        "\n",
        "def random_bools(sample_size, n):\n",
        "\n",
        "  temp =  np.random.random_integers(0, high=1, size=[sample_size, n])\n",
        "  temp = temp*2 - 1\n",
        "  return temp.astype(np.float32)\n",
        "  \n",
        "\n",
        "def model(collection, message, key=None):\n",
        "\n",
        "  if key is not None:\n",
        "    combined_message = tf.concat(axis=1, \n",
        "      values=[message, key])\n",
        "  else:\n",
        "    combined_message = message\n",
        "\n",
        "  with tf.variable_scope(collection):\n",
        "    fc = tf.layers.dense(combined_message, TEXT_SIZE + KEY_SIZE, activation=tf.nn.relu)\n",
        "    fc = tf.expand_dims(fc, 2)\n",
        "\n",
        "    # tf.contrib.layers.conv1d( input, filters, kernel_size, stride, padding, activation_fn)\n",
        "    conv1 = tf.layers.conv1d( fc, filters= 2, kernel_size= 4, strides= 1, padding='SAME',  activation=tf.nn.sigmoid)\n",
        "    conv2 = tf.layers.conv1d( conv1, filters= 4, kernel_size= 2, strides=2, padding='VALID', activation=tf.nn.sigmoid)\n",
        "    conv3 = tf.layers.conv1d( conv2, filters= 4, kernel_size= 1, strides=1, padding='SAME',  activation=tf.nn.sigmoid)\n",
        "\n",
        "    # output\n",
        "    conv4 = tf.layers.conv1d( conv3, filters= 1, kernel_size= 1, strides=1, padding='SAME',  activation=tf.nn.tanh)\n",
        "\n",
        "\n",
        "    out   = tf.squeeze(conv4, 2)\n",
        "  return out\n",
        "\n",
        "\n",
        "\n",
        "Alice_input_message  = tf.placeholder(tf.float32, shape=(batch_size, TEXT_SIZE), name='Alice_input_message')\n",
        "Alice_input_key      = tf.placeholder(tf.float32, shape=(batch_size, KEY_SIZE), name='Alice_input_key')\n",
        "\n",
        "\n",
        "\n",
        "Alice_out_cipher = model('Alice', Alice_input_message, Alice_input_key)\n",
        "Bob_out_message  = model('Bob', Alice_out_cipher, Alice_input_key)\n",
        "Eve_out_message  = model('Eve', Alice_out_cipher)\n",
        "\n",
        "\n",
        "## Eves LOSS\n",
        "Eves_loss = (1/batch_size)*tf.reduce_sum( tf.abs( Eve_out_message - Alice_input_message ))\n",
        "\n",
        "## ALICE AND BOB LOSS\n",
        "Bob_loss = (1/batch_size)*tf.reduce_sum( tf.abs( Bob_out_message  - Alice_input_message ))\n",
        "Eve_evadropping_loss = tf.reduce_sum( tf.square(float(TEXT_SIZE) / 2.0 - Eves_loss) / ((TEXT_SIZE / 2)**2) )\n",
        "\n",
        "Alice_bob_loss = Bob_loss + Eve_evadropping_loss\n",
        "\n",
        "\n",
        "\n",
        "# Get tensors to train\n",
        "Alice_vars =  tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope='Alice') \n",
        "Bob_vars   =  tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope='Bob') \n",
        "Eve_vars   =  tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Eve') \n",
        "\n",
        "Eve_opt  = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.9, epsilon=1e-08).minimize(Eves_loss, var_list=[Eve_vars])\n",
        "bob_opt  = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.9, epsilon=1e-08).minimize(Alice_bob_loss, var_list=[Alice_vars + Bob_vars])\n",
        "\n",
        "\n",
        "sess = tf.Session() \n",
        "init = tf.global_variables_initializer()\n",
        "sess.run(init)\n",
        "\n",
        "alice_saver = tf.train.Saver(Alice_vars)\n",
        "bob_saver   = tf.train.Saver(Bob_vars)\n",
        "eve_saver   = tf.train.Saver(Eve_vars)\n",
        "\n",
        "\n",
        "if restore_trained_model:\n",
        "  alice_saver.restore(sess, \"weights/alice_weights/model.ckpt\")\n",
        "  bob_saver.restore(sess, \"weights/bob_weights/model.ckpt\")\n",
        "  eve_saver.restore(sess, \"weights/eve_weights/model.ckpt\")\n",
        "\n",
        "\n",
        "# DATASET \n",
        "messages = random_bools(sample_size, TEXT_SIZE)\n",
        "keys     = random_bools(sample_size, KEY_SIZE)\n",
        "\n",
        "\n",
        "# Training begins\n",
        "for i in range(epochs):\n",
        "\n",
        "  for j in range(steps_per_epoch):\n",
        "\n",
        "    # get batch dataset to train\n",
        "    batch_messages = messages[j*batch_size: (j+1)*batch_size]\n",
        "    batch_keys     = keys[j*batch_size: (j+1)*batch_size]\n",
        "\n",
        "    # Train Alice and Bob\n",
        "    for _ in range(ITERS_PER_ACTOR):\n",
        "      temp = sess.run([bob_opt, Bob_loss, Eve_evadropping_loss, Bob_out_message],feed_dict={Alice_input_message:batch_messages , Alice_input_key:batch_keys })\n",
        "      \n",
        "      temp_alice_bob_loss = temp[1]\n",
        "      temp_eve_evs_loss   = temp[2]\n",
        "      temp_bob_msg        = temp[3]\n",
        "\n",
        "    # train Eve\n",
        "    for _ in range(ITERS_PER_ACTOR*EVE_MULTIPLIER):\n",
        "      temp = sess.run([Eve_opt, Eves_loss, Eve_out_message], feed_dict={Alice_input_message:batch_messages , Alice_input_key:batch_keys })\n",
        "\n",
        "      temp_eve_loss = temp[1]\n",
        "      temp_eve_msg  = temp[2]\n",
        "\n",
        "\n",
        "  # output bit error and loss after every 100 epochs\n",
        "  if i%50 == 0:\n",
        "    print('  epochs: ', i, '  bob bit error: ', temp_alice_bob_loss,' + ', temp_eve_evs_loss,'   & eve bit error:', temp_eve_loss)\n",
        "\n",
        "sess.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}